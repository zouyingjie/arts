### 一. Algorithm

做了 [200. Number of Islands](https://leetcode.com/problems/number-of-islands/)，计算岛屿数量，一道非常经典的考察 DFS 的题目。

题目要求：

- 给出一个二维数组，1 代表陆地，0 代表海洋，互相关联的 1 视为同一块陆地，找出所有的陆地数量。

思路：

- 遍历数组，找出值为 '1' 的元素
- 以该元素为开始，遍历其所有相邻的值为 '1' 的元素
- 已遍历元素全部设置为 '0'，防止重复遍历

代码如下：

```Java
class Solution {
     private int result = 0;
    private char[][] grid = null;
    private int row = 0;
    private int column = 0;

    public int numIslands(char[][] grid) {
        this.grid = grid;
        this.row = grid.length;
        if (row == 0) {
            return row;
        }
        this.column = grid[0].length;

        for (int i = 0; i < row; i ++) {
            for (int j = 0; j < column; j ++) {
                if (grid[i][j] == '1') {
                    result ++;
                    dfs(i, j);
                }else {
                    continue;
                }

            }
        }
        return result;
    }

    private void dfs(int i, int j) {

        if (i >= row || j >= column | i < 0 | j < 0) {
            return;
        }
        if (grid[i][j] == '1') {
            // 已经遍历到的岛屿置为 0，避免重复计算
            grid[i][j] = '0';
            // 可能会导致重复遍历，但确实是有必要的。
            dfs(i +1, j);
            dfs(i -1, j);
            dfs(i , j+1);
            dfs(i , j-1);
        }
    }
}
```

乍一看挺麻烦的题目，但是想清楚考察目的以及掌握 DFS 后还是挺好解决的，每个元素都会被遍历到，时间复杂度为 O(N)，实际运行 3ms，beats 99%，性能基本及格。

### 二. Review

读了耗子叔练级攻略中推荐的一篇一致性哈希相关的文章：[Consistent Hashing](http://www.tom-e-white.com/2007/11/consistent-hashing.html)。简要总结下其内容。


#### 1. 起源

一致性哈希的提出来源于 MIT的Karger 的一篇论文： [Consistent Hashing and Random Trees: Distributed Caching Protocols for Relieving Hot Spots on the World Wide Web ](http://citeseer.ist.psu.edu/karger97consistent.html)，现在在亚马逊的 Dynamo 项目和热门的开源组件 memcached 中有了广泛的使用。

一致性哈希主要是用于解决在分布式系统，尤其是分布式缓存系统中，当缓存节点发生变化时数据映射的一致性问题。举个例子，对于有 N 个节点的分布式缓存系统，如果使用传统的哈希算法进行节点映射，对于键 key 要存储的节点，其计算公式如下：

```
node = hash(key) % N
```

但是当节点被增加或者删除时，N 发生变化，那么上述公式的计算结果就会出错，导致已经缓存好的数据无法被访问到，如果数据量过大的话会导致大量请求发送到数据库，导致应用崩溃。

因为在分布式缓存系统中，其映射算法必须保证：

- 当新增节点时，数据依旧会均匀的分配到各个节点中，并且已有的缓存数据不受影响
- 当删除节点时，除删除节点之外的缓存数据也会被正常访问。

#### 2. 实现

一致性哈希基本的思路是使用相同的哈希算法对缓存节点和存储对象进行处理。

如图所示，我们有 A、B、C 3 个缓存节点，1，2，3，4 四个要存储的数据。可以将所有的缓存空间视为一个环，使用相同的哈希算法处理后，节点和数据落在了整个环上，然后以顺时针的顺序查找数据，那么 2 就存储在 B 节点，3 存储在 C 节点，1、4 存储在 A 节点。

![](http://2.bp.blogspot.com/_IhqEHw4_Ick/Rz9cjSPnAEI/AAAAAAAAAA4/hc2tot8SWVw/s400/consistent_hashing_1.png)

此时如果将 C 节点移除，那么包括 3 在内的本来 B/C 之间的数据将会落到 A 节点，其他节点的数据存储不受影响。如果我们新加一个 D 节点，如图：

![](http://4.bp.blogspot.com/_IhqEHw4_Ick/Rz9cwyPnAFI/AAAAAAAAABA/aW5zxmOIIN0/s400/consistent_hashing_2.png)


此时 3、4 将会落到 D 节点中，而 1 依然属于 A 节点。通过这种方式，一致性哈希保证了当节点发生变化时，只有少数数据受到影响。

另外当节点数量过少的时候可能出现数据分布不均的情况，可以采用 ”virtual nodes“ 虚拟节点的方式，基于物理节点映射出若干个子节点，然后把子节点映射到整个环境空间中，尽可能保证数据的均匀分布。


下图是作者做的一个测试，数据分布与节点数量之间的关系，横轴是节点数量，纵轴是数据分布情况的标准差，越低表示分布越均匀。可以看出随着节点增加数据分布趋向于平衡。

![](http://1.bp.blogspot.com/_IhqEHw4_Ick/Rz9daCPnAGI/AAAAAAAAABI/xjtbuG8Knx0/s400/ch-graph.png)

下面是作者给出的一段示例代码：

```Java
import java.util.Collection;
import java.util.SortedMap;
import java.util.TreeMap;

public class ConsistentHash<T> {

  private final HashFunction hashFunction;

  // 节点副本数，也就是虚拟节点数
  private final int numberOfReplicas;

  // 代表节点缓存节点
  private final SortedMap<Integer, T> circle =
    new TreeMap<Integer, T>();

  public ConsistentHash(HashFunction hashFunction,
    int numberOfReplicas, Collection<T> nodes) {

    this.hashFunction = hashFunction;
    this.numberOfReplicas = numberOfReplicas;

    for (T node : nodes) {
      add(node);
    }
  }

  
  // 添加节点，基于副本数添加，保证数据的均匀分布
  public void add(T node) {
    for (int i = 0; i < numberOfReplicas; i++) {
      circle.put(hashFunction.hash(node.toString() + i),
        node);
    }
  }

  // 删除节点，同时删除所有副本
  public void remove(T node) {
    for (int i = 0; i < numberOfReplicas; i++) {
      circle.remove(hashFunction.hash(node.toString() + i));
    }
  }

  // 获取存储数据的节点
  public T get(Object key) {
    if (circle.isEmpty()) {
      return null;
    }
    int hash = hashFunction.hash(key);
    // 节点被移除
    if (!circle.containsKey(hash)) {
      // 选择新的节点
      SortedMap<Integer, T> tailMap =
        circle.tailMap(hash);
      hash = tailMap.isEmpty() ?
             circle.firstKey() : tailMap.firstKey();
    }
    return circle.get(hash);
  } 

}
```

以上就是文章的主要内容，一般来说我们不会在实际工作中自己编写相关算法，不过了解其原理特性还是非常有必要的。另外推荐一遍非常有意思的文章：

[漫画算法：什么是一致性哈希？](https://www.jianshu.com/p/570dc8913c20)，图文并茂，浅显易懂。


### 三. Tips


最近遇到一个需求，需要将图片用 base64 编码之后用 url 传输，直接传的话会遇到问题，服务端在解码的时候会报错，主要原因是：

Base64 编码中含有 +，在使用 application/x-www-form-urlencoded 传参方式时，会将 + 加号变为空格，导致服务端收到的 base64 不准确。 

为了解决该问题有几种方式：

#### 1. Python 的 urlsafe 中处理

在 Python 中为了保证 base64 的安全传输，其 base64 模块默认提供处理方式：

```Python
s = "123"

// base64 编码
str_b64 = base64.urlsafe_b64encode(s)

// base64 解码
s = base64.urlsafe_b64decode(str_b64)

```
 
 Python 的 urlsafe_b64encode 会将 base64 字符串中的 + 替换为 -，/ 替换为 "_"，从而保证在参数传递时不会出错。

 #### 2. 服务器端做替换处理

上面是 Python 的处理方式，不是很通用，在看一下更为通用的方式。为了保证所有 base64 字段传递的正确性，服务器端可以在接收到 base64 类型的字段后做一次统一的处理，对于 + 变为空格的情况，将参数中的空格在替换为 + 加号即可。这种方式对于其他有类似传参问题的情况同样适用。

 ```Java
 s.replaceAll(" ", "+")
 ```

 #### 3. 加密传输

 问题的根源在于 url 将 + 号变为空格，那么在传参之前提前做一次 urlencode 将 + 进行编码，此时服务器端接收到参数就不会有空格，在首先进行一次 urldecode 在做 base64 解码就不会有问题了。


  ```Java
// 发送端
ls_f = base64.b64encode(BytesIO(response.content).read())
return urllib.quote_plus(ls_f)

// 服务端
s = urllib.unquote_plus(params)
ls_f = base64.b64decode(s)
```

### 四. Share

偶然读了《5 分钟商学院》刘润大佬的一篇文章：

[一切商业规则的背后，都是有漏洞的](https://weibo.com/ttarticle/p/show?id=2309404354224434616079#_0)。

文章的观点很明确：

> 这个世界上没有完美的规则。不管我们再怎么精心设计，一切规则的背后，都是有漏洞的。 如果你非要找一个十全十美的制度，那不是勇敢，而是无知。

> 果目前有一个不错的规则、制度和方法论，那就先用着吧，先好好享受它的好处。遇到漏洞，就弥补漏洞，遇到问题，再解决问题。直到有一天，这个规则、制度和方法论不再适用，你可以再选择放弃它。

不仅是商业规则，技术方案架构、生活亦是如此，没有十全十美的技术方案，所有的技术方案都是一种 trade-off，重要的不是完美，而是在解决当下问题的前提下，尽可能的在做一点长远、优雅的改进。

自己在开发中就经常遇到这个问题，在考虑某个问题的解决方案时，如果每个方案都会有一个问题，解决某个问题后还会有潜在的风险，自己往往会陷入不知所措的窘境。过度的追求一劳永逸，不留问题反而浪费了过多的时间。其实在经过充分论证讨论的基础之上，在所有可选择方案中选择最合适的一个，只要这个方案能解决当下问题，并且预见到的问题可控，那就可以实施了，遇到问题的时候在解决问题，甚至采用新的架构方案就可以了。

针对当下的解决方案，最重要的是两点：

- 合适： 选择的方案应该是在符合当前约束条件下的最合适的方案
- 预见性：可以想象系统将来要变成的样子，然后如果要变成这个样子，我们现在应该怎样做。由远及近的做推演，从而不至于过于沉溺于当下而忽略了未来的发展。
